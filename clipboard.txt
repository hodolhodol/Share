# ì¼ë°˜ ë§í¬ ë³€ê²½ + ì§§ì€ URL ìˆ˜ì§‘

import requests, re, csv, time, certifi, urllib.parse, json
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from drawio_utils import replace_links_drawio
import os
# ì„¤ì •
#ì„¤ì • - ê²€ì¦ì„œë²„
load_dotenv()

EMAIL = os.getenv("EMAIL")
API_TOKEN = os.getenv("API_TOKEN")


BASE_URL = "https://devops-qa.martin.co.kr/confluence"   # ë˜ëŠ” ë‚´ë¶€ ë„ë©”ì¸
PAGE_ID = "823654206"  # í…ŒìŠ¤íŠ¸í•  Confluence í˜ì´ì§€ ID
CERT_PATH = "..."
ROOT_PAGE_ID = "1066435477"  # í…ŒìŠ¤íŠ¸í•  Confluence í˜ì´ì§€ ID

# ORIGIN_SPACES = ['TR', 'AGILEK', 'DCO']
# TARGET_SPACE = 'Knowledge'
ORIGIN_SPACES = ['TR', 'AGILEK', 'DCO']
TARGET_SPACE = 'ARU'

TESTPAGE = 'TESTPAGE'

auth = (EMAIL, API_TOKEN)
headers = {
    "Authorization": f"Bearer {API_TOKEN}",  # Bearer í† í° ë°©ì‹ìœ¼ë¡œ ì¸ì¦
   "Content-Type": "application/json",
#    "Accept": "application/json"
}

short_urls = {}
pageid_urls = {}



def get_all_page_ids(space_key):
    ids = []
    start = 0
    while True:
        url = f"{BASE_URL}/rest/api/content?spaceKey={space_key}&limit=50&start={start}&expand=version"
        res = requests.get(url, headers=headers)
        results = res.json().get("results", [])
        if not results: break
        for page in results:
            ids.append((page['id'], page['title']))
        start += 50
    return ids
def get_child_pages(parent_id):
    """ì§€ì •í•œ í˜ì´ì§€ ID ì´í•˜ì˜ ëª¨ë“  í•˜ìœ„ í˜ì´ì§€ ID+ì œëª© ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    pages = []
    stack = [parent_id]

    while stack:
        current_id = stack.pop()
        url = f"{BASE_URL}/rest/api/content/{current_id}?expand=children.page"
        #res = requests.get(url, auth=auth)
        res = requests.get(url, headers=headers)
        if res.status_code != 200:
            print(f"âŒ Failed to get children of {current_id}")
            continue

        data = res.json()
        title = data.get("title", "Untitled")
        pages.append((current_id, title))

        children = data.get("children", {}).get("page", {}).get("results", [])
        for child in children:
            stack.append(child["id"])
    return pages
  
def replace_links_spacekey(body):
    for space in ORIGIN_SPACES:
        #body = re.sub(rf'{BASE_URL}/display/{space}/', f'{BASE_URL}/display/{TARGET_SPACE}/', body)
        body = re.sub(rf'{BASE_URL}/display/{space}/', f'{BASE_URL}/display/{TARGET_SPACE}/', body)         
        body = re.sub(rf'/display/{space}/', f'{BASE_URL}/display/{TARGET_SPACE}/', body)   
        

        body = re.sub(rf'/spaces/{space}/pages/', f'/spaces/{TARGET_SPACE}/pages/', body)
        body = re.sub(rf'https://[^"]+/wiki/display/{space}/', f'/display/{TARGET_SPACE}/', body)
        body = re.sub(rf'https://[^"]+/wiki/spaces/{space}/pages/', f'/spaces/{TARGET_SPACE}/pages/', body)
        body = re.sub(rf'link="[^"]*/spaces/{space}/pages/', lambda m: m.group(0).replace(f'/spaces/{space}/', f'/spaces/{TARGET_SPACE}/'), body)
    return body

def replace_links_tinyui(body, title, page_id):
    matches = re.findall(rf'{BASE_URL}/x/[a-zA-Z0-9]+', body)
    for m in matches:
        partial = "".join(m)
        short_url = f"{BASE_URL}{partial}" if partial.startswith('/x') or '/wiki/x' in partial else partial
        if short_url not in short_urls:
            new_url = get_new_short_url(short_url, TARGET_SPACE)
            short_urls[short_url] = new_url
            body = re.sub(short_url, new_url, body)
            print(f"ğŸ”— Replaced {short_url} with {new_url}")

        # else:
        #     print(f"âŒ ì´ short urlì€ ìˆ˜ì •ë˜ì—ˆì–´ì•¼ í•¨ {short_url}")
        #    continue
    
    return body

def replace_links_page_id(body):
    matches = re.findall(rf'{BASE_URL}/pages/viewpage\.action\?pageId=\d+', body)
    for m in matches:
        page_id = extract_page_id(m)
        if page_id in pageid_urls:
            continue
#        if page_id == pageid_urls[page_id] : continue #page_id ë™ì¼í•˜ë©´ spaceê°€ origin_spaceì— ìˆë˜ê²Œ ì•„ë‹ˆë‹¤. ì¦‰, ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤.
        page_info = get_page_info_by_id(page_id)
        if page_info is None:
            print(f"âŒ Page not found: {page_id}")
            continue
        space = page_info['_expandable']['space'].strip('/').split('/')[-1] #space pathì˜ ë§¨ë§ˆì§€ë§‰ ê°€ì§€ê³  ì˜´
        title = page_info['title']
        if space in ORIGIN_SPACES:
                target_page_info = get_page_info_by_title(TARGET_SPACE, title)
                if target_page_info:
                    target_page_id = target_page_info.get('id')
                    old_url = m
                    new_url = f"{BASE_URL}/pages/viewpage.action?pageId={target_page_id}"
                    body = body.replace(old_url, new_url)
                    print(f"ğŸ”— Replaced {old_url} with {new_url}")
                    pageid_urls[page_id] = target_page_id
        else :
            pageid_urls[page_id] = page_id #page_id ë™ì¼í•˜ë©´ spaceê°€ origin_spaceì— ìˆë˜ê²Œ ì•„ë‹ˆë‹¤. ì¦‰, ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤.

    return body


def resolve_short_url_to_title(short_url):
    """
    short URLì„ í’€ì–´ì„œ page ID ë°˜í™˜
    :param short_url: e.g. https://your-domain.atlassian.net/x/AbCdE
    :param auth: requests basic auth tuple (username, API token)
    """
    response = requests.get(short_url, headers=headers, allow_redirects=True)
    
    # ìµœì¢… ë¦¬ë””ë ‰ì…˜ URLì—ì„œ page ID ì¶”ì¶œ
    title = response.url.split('/')[-1].replace('+', ' ') # urlì—ì„œëŠ” ìŠ¤í˜ì´ìŠ¤ê°€ +ë¡œ ë‚˜ì™€ì„œ.
    return title
    
    return title

    
def resolve_tiny_url(short_url):
    response = requests.get(short_url, allow_redirects=False, headers=headers)
    if response.status_code in [301, 302]:
        return response.headers['Location']
    else:
        raise Exception(f"ë¦¬ë””ë ‰ì…˜ ì‹¤íŒ¨: status code {response.status_code}")

# 2. URLì—ì„œ page ID ì¶”ì¶œ (e.g. /pages/viewpage.action?pageId=12345678)
def extract_page_id(full_url):
    match = re.search(r"pageId=(\d+)", full_url)
    if match:
        return match.group(1)
    else:
        raise Exception(f"pageIdë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_url}")
    
def get_page_info_by_id(page_id):
    """
    page IDë¡œ title ë“± í˜ì´ì§€ ì •ë³´ ì¡°íšŒ
    """
    url = f"{BASE_URL}/rest/api/content/{page_id}"
    params = {"expand": "title"}
    response = requests.get(url, headers=headers, params=params)
    response.raise_for_status()
    return response.json()
    


def get_short_url_by_title(title, space_key, base_url):
    """
    ì£¼ì–´ì§„ titleë¡œ í˜ì´ì§€ë¥¼ ê²€ìƒ‰í•˜ê³  short URL(tiny link)ì„ ë°˜í™˜
    """
    # 1. ì œëª©ìœ¼ë¡œ í˜ì´ì§€ ì¡°íšŒ
    url = f"{base_url}/rest/api/search"
    params = {
        "title": title,
        "space": space_key,
        "expand": "version"  # title ì¡´ì¬ ìœ ë¬´ í™•ì¸ìš©
    }
    resp = requests.get(url, headers=headers, params=params)
    resp.raise_for_status()
    data = resp.json()
    
    # 2. ê²°ê³¼ ì—†ì„ ê²½ìš° ì²˜ë¦¬
    if not data.get("results"):
        print("Martin", "get_short_url_by_title", "í•´ë‹¹ í˜ì´ì§€ ì—†ìŒ {title}")
        return None  # or raise Exception("Page not found")
    
    page = data["results"][0]
    page_id = page["id"]
    
    # 3. í˜ì´ì§€ IDë¡œ tiny link ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    url = f"{base_url}/rest/api/content/{page_id}?expand=shortUrl,tinyui"
    resp = requests.get(url, auth=auth)
    resp.raise_for_status()
    page_data = resp.json()

    # 4. shortUrl í•„ë“œê°€ ì¡´ì¬í•  ê²½ìš° ë°˜í™˜
    tiny_url = page_data.get("tinyui", {}).get("link")
    if tiny_url:
        return f"{base_url}{tiny_url}"
    else:
        return None
    
def url_encode_query(query_string):
    """
    Encodes the given query string for use in a URL.

    Args:
    query_string (str): The query string to encode.

    Returns:
    str: The URL-encoded query string.
    """
    return urllib.parse.quote_plus(query_string)

def get_page_info_by_title(space_key, title):
    url = f"{BASE_URL}/rest/api/search"
    params = {
        'cql': f'title="{title}" AND space="{space_key}"',
        'limit': 10
    }
    
    response = requests.get(url, headers=headers, params=params)

    if response.status_code != 200:
        raise Exception(f"Request failed with status code {response.status_code}")
    
    data = response.json()
    results = data.get('results', [])
    
    if not results:
        return None
    
    for result in results:
        if result.get('title') == title:
            return {
                "id" :  result['content']['id'],
                "title" :  result['content']['title'],
                "webui" :  result['content']['_links']['webui'],
                "tinyui" :  result['content']['_links']['tinyui']
            }
    return None


def get_new_short_url(short_url, new_space):
    # Step 1: Extract the page ID from the short URL
    title = resolve_short_url_to_title(short_url)
    new_short_url = get_page_info_by_title(TARGET_SPACE, title)['tinyui']   

    if new_short_url:
        return f"{BASE_URL}{new_short_url}"
    else:
        return None
        

def update_page(pid, title):
    url = f"{BASE_URL}/rest/api/content/{pid}?expand=body.storage,version"
    res = requests.get(url, headers=headers)
    if res.status_code != 200:
        print(f"âŒ Failed to get {title}")
        return

    data = res.json()
    body = data['body']['storage']['value']
    version = data['version']['number']
    
    new_body = replace_links_spacekey(body)
    new_body = replace_links_tinyui(new_body, title, pid)
    new_body = replace_links_page_id(new_body)
    new_body = replace_links_drawio(newbody, data)
    if new_body == body:
        print(f"ğŸ” No change: {title}")
        return


    space = data['_expandable']['space'].strip('/').split('/')[-1] #space pathì˜ ë§¨ë§ˆì§€ë§‰ ê°€ì§€ê³  ì˜´
    payload = {
        "id": pid,
        "type": "page",
        "title": title,
        "space": {"key": space},
        # "body": {"storage": {"value": "hello world", "representation": "storage"}},
        "body": {"storage": {"value": new_body, "representation": "storage"}},
        "version": {"number": version + 1}
    }

    put_url = f"{BASE_URL}/rest/api/content/{pid}"
    put_res = requests.put(put_url, json=payload, headers=headers)
    print(f"{'âœ… Updated' if put_res.status_code == 200 else 'âŒ Failed'}: {title}")

def set_variables(mode) :
    global BASE_URL, PAGE_ID, ORIGIN_SPACES, TARGET_SPACE
    if mode == "TEST" :
        BASE_URL = "https://devops-qa.martin.co.kr/confluence"
        PAGE_ID = "1127350378"
        ORIGIN_SPACES = ['TPG']
        TARGET_SPACE = 'ARU'

if __name__ == "__main__":
#    test_short_url()
    set_variables("TEST")
    update_page(PAGE_ID, TESTPAGE)

    # pages = get_child_pages(ROOT_PAGE_ID)
    # print(f"ğŸ” Pages under root {ROOT_PAGE_ID}: {len(pages)}")

    # for pid, title in pages:
    #     update_page(pid, title)
    #     time.sleep(0.5)

    filename = 'short_urls.csv'
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        fieldnames = ['old_url', 'new_url']
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        for old_url, new_url in short_urls.items():
            writer.writerow({'old_url': old_url, 'new_url': new_url})


    print(f"\nâš ï¸ Short URLs written to short_urls.csv: {len(short_urls)}")
