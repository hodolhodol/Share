# ì¼ë°˜ ë§í¬ ë³€ê²½ + ì§§ì€ URL ìˆ˜ì§‘

import requests, re, csv, time, certifi, urllib.parse, json
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from typing import List, Tuple, Optional, Callable, Dict, Any
from urllib.parse import urljoin, urlparse, parse_qs, unquote_plus
#from drawio_utils import replace_links_drawio
import os
# ì„¤ì •
#ì„¤ì • - ê²€ì¦ì„œë²„
load_dotenv()

EMAIL = os.getenv("EMAIL")
API_TOKEN = os.getenv("API_TOKEN")
CERT_PATH = os.getenv("CERT_PATH")


BASE_URL =  os.getenv("BASE_URL") # ë˜ëŠ” ë‚´ë¶€ ë„ë©”ì¸
TEST_BASE_URL = os.getenv("TEST_BASE_URL") # ë˜ëŠ” ë‚´ë¶€ ë„ë©”ì¸
PAGE_ID = "823654206"  # í…ŒìŠ¤íŠ¸í•  Confluence í˜ì´ì§€ ID

ROOT_PAGE_ID = "1066435477"  # í…ŒìŠ¤íŠ¸í•  Confluence í˜ì´ì§€ ID

# ORIGIN_SPACES = ['TR', 'AGILEK', 'DCO']
# TARGET_SPACE = 'Knowledge'
ORIGIN_SPACES = ['TR', 'AGILEK', 'DCO']
TARGET_SPACE = 'ARU'

TESTPAGE = 'TESTPAGE'

auth = (EMAIL, API_TOKEN)
headers = {
    "Authorization": f"Bearer {API_TOKEN}",  # Bearer í† í° ë°©ì‹ìœ¼ë¡œ ì¸ì¦
   "Content-Type": "application/json",
#    "Accept": "application/json"
}

short_urls = {}
pageid_urls = {}



def get_all_page_ids(space_key):
    ids = []
    start = 0
    while True:
        url = f"{BASE_URL}/rest/api/content?spaceKey={space_key}&limit=50&start={start}&expand=version"
        res = requests.get(url, headers=headers)
        results = res.json().get("results", [])
        if not results: break
        for page in results:
            ids.append((page['id'], page['title']))
        start += 50
    return ids
def get_child_pages(parent_id):
    """ì§€ì •í•œ í˜ì´ì§€ ID ì´í•˜ì˜ ëª¨ë“  í•˜ìœ„ í˜ì´ì§€ ID+ì œëª© ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    pages = []
    stack = [parent_id]

    while stack:
        current_id = stack.pop()
        url = f"{BASE_URL}/rest/api/content/{current_id}?expand=children.page"
        #res = requests.get(url, auth=auth)
        res = requests.get(url, headers=headers)
        if res.status_code != 200:
            print(f"âŒ Failed to get children of {current_id}")
            continue

        data = res.json()
        title = data.get("title", "Untitled")
        pages.append((current_id, title))

        children = data.get("children", {}).get("page", {}).get("results", [])
        for child in children:
            stack.append(child["id"])
    return pages
  
def replace_links_spacekey(body, prefix=""):
    for space in ORIGIN_SPACES:
        #body = re.sub(rf'{BASE_URL}/display/{space}/', f'{BASE_URL}/display/{TARGET_SPACE}/', body)
        body = re.sub(rf'{prefix+BASE_URL}/display/{space}/', f'{prefix+BASE_URL}/display/{TARGET_SPACE}/', body)         
        body = re.sub(rf'{prefix}/display/{space}/', f'{prefix+BASE_URL}/display/{TARGET_SPACE}/', body)   
        

        body = re.sub(rf'{prefix}/spaces/{space}/pages/', f'{prefix}/spaces/{TARGET_SPACE}/pages/', body)
        body = re.sub(rf'{prefix}https://[^"]+/wiki/display/{space}/', f'{prefix}/display/{TARGET_SPACE}/', body)
        body = re.sub(rf'{prefix}https://[^"]+/wiki/spaces/{space}/pages/', f'{prefix}/spaces/{TARGET_SPACE}/pages/', body)
        body = re.sub(rf'link="[^"]*/spaces/{space}/pages/', lambda m: m.group(0).replace(f'/spaces/{space}/', f'/spaces/{TARGET_SPACE}/'), body)
    return body

def replace_links_tinyui(body, page_id, prefix=""):
    matches = re.findall(rf'{prefix+BASE_URL}/x/[a-zA-Z0-9]+', body)
    for m in matches:
        partial = "".join(m)
        short_url = f"{BASE_URL}{partial}" if partial.startswith('/x') or '/wiki/x' in partial else partial
        if short_url not in short_urls:
            new_url = get_new_short_url(short_url, TARGET_SPACE)
            short_urls[short_url] = new_url
            body = re.sub(short_url, new_url, body)
            print(f"ğŸ”— Replaced {short_url} with {new_url}")

        # else:
        #     print(f"âŒ ì´ short urlì€ ìˆ˜ì •ë˜ì—ˆì–´ì•¼ í•¨ {short_url}")
        #    continue
    
    return body


def replace_links_short_page_id(body, prefix=""):
    return replace_links_page_id(body, "", "")


def replace_links_page_id(body, prefix="", base_url=BASE_URL):
    matches = re.findall(rf'{prefix+base_url}/pages/viewpage\.action\?pageId=\d+', body)
    for m in matches:
        page_id = extract_page_id(m)
        if page_id in pageid_urls:
            continue
#        if page_id == pageid_urls[page_id] : continue #page_id ë™ì¼í•˜ë©´ spaceê°€ origin_spaceì— ìˆë˜ê²Œ ì•„ë‹ˆë‹¤. ì¦‰, ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤.
        page_info = get_page_info_by_id(page_id)
        if page_info is None:
            print(f"âŒ Page not found: {page_id}")
            continue
        space = page_info['_expandable']['space'].strip('/').split('/')[-1] #space pathì˜ ë§¨ë§ˆì§€ë§‰ ê°€ì§€ê³  ì˜´
        title = page_info['title']
        if space in ORIGIN_SPACES:
                target_page_info = get_page_info_by_title(TARGET_SPACE, title)
                if target_page_info:
                    target_page_id = target_page_info.get('id')
                    old_url = m
                    new_url = f"{base_url}/pages/viewpage.action?pageId={target_page_id}"
                    body = body.replace(old_url, new_url)
                    print(f"ğŸ”— Replaced {old_url} with {new_url}")
                    pageid_urls[page_id] = target_page_id
        else :
            pageid_urls[page_id] = page_id #page_id ë™ì¼í•˜ë©´ spaceê°€ origin_spaceì— ìˆë˜ê²Œ ì•„ë‹ˆë‹¤. ì¦‰, ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤.

    return body

def _list_attachments(page_id: str, limit: int = 500):
    url = f"{BASE_URL}/rest/api/content/{page_id}/child/attachment?limit={limit}&expand=metadata.labels,metadata.mediaType"
    res = requests.get(url, headers=headers)
    res.raise_for_status()
    return res.json().get("results", [])

def _download_attachment_via_link(att: Dict[str, Any]):
    """
    ì²¨ë¶€ ê°ì²´ì˜ _links.download ë¥¼ ì‚¬ìš©í•´ ì•ˆì „í•˜ê²Œ ë‹¤ìš´ë¡œë“œ.
    base_url ì€ ì»¨í…ìŠ¤íŠ¸(/confluence, /wiki í¬í•¨)ê¹Œì§€ ë“¤ì–´ê°„ ê°’ì„ ë„˜ê¸°ì„¸ìš”.
    """
    links = att.get("_links", {}) or {}
    dl_path = links.get("download")  # ì˜ˆ: "/download/attachments/12345/diagram?version=2&api=v2"
    if not dl_path:
        raise RuntimeError(f"No download link in attachment: {att.get('id')}")

    url = urljoin(BASE_URL if BASE_URL.endswith("/") else BASE_URL + "/", dl_path.lstrip("/"))
    res = requests.get(url, headers=headers, allow_redirects=True)
    
    if res.status_code == 404:
        # ì¼ë¶€ ì¸ìŠ¤í„´ìŠ¤ëŠ” ì»¨í…ìŠ¤íŠ¸ ê²½ë¡œ ì´ìŠˆë¡œ ë£¨íŠ¸(host) ê¸°ì¤€ì´ í•„ìš”í•œ ê²½ìš°ê°€ ìˆìŒ
        from urllib.parse import urlparse
        p = urlparse(BASE_URL)
        host_root = f"{p.scheme}://{p.netloc}"
        url2 = urljoin(host_root + "/", dl_path.lstrip("/"))
        res = requests.get(url2, headers=headers, allow_redirects=True)
        

    res.raise_for_status()
    return res.content, res.headers.get("Content-Type", "")

# ========= draw.io íŒŒì¼ ì²˜ë¦¬ =========
def _looks_like_mxfile(data: bytes) -> bool:
    # ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±: <mxfile â€¦> í—¤ë” í™•ì¸
    head = data[:2048].decode("utf-8", errors="ignore")
    return "<mxfile" in head

def _rewrite_single_url(old_url: str) -> Optional[str]:
    """
    old_url â†’ (ORIGIN_SPACES â†’ TARGET_SPACE ë™ì¼ ì œëª©) ìƒˆ URL.
    ë§¤í•‘ ì‹¤íŒ¨ ì‹œ None.
    """
    # try:
    #     u = _normalize_url(old_url, base_url)
    #     pid = _extract_pageid_from_url(u, session, base_url)
    #     if pid:
    #         new_u = _build_target_url_by_pageid(session, base_url, origin_spaces, target_space, pid)
    #         if new_u:
    #             return new_u

    #     # pageId ë¯¸ê²€ì¶œ: /display/ORG/TITLE ì§ì ‘ ë§¤í•‘
    #     p = urlparse(u)
    #     m = re.search(r"/display/([^/]+)/(.+)$", p.path or "")
    #     if m and m.group(1) in origin_spaces:
    #         title = _decode_title_slug(m.group(2))
    #         tgt = _build_target_url_by_title(session, base_url, target_space, title)
    #         if tgt:
    #             return tgt

    #     return None
    # except Exception:
    #     return None

def _upload_new_attachment_version( page_id: str,
                                   attachment_id: str, filename: str, data: bytes, content_type: str):
    url = f"{BASE_URL}/rest/api/content/{page_id}/child/attachment/{attachment_id}/data"
    files = {'file': (filename or "diagram.drawio", data, content_type or 'application/octet-stream')}
    
    r = requests.post(url, headers=headers, files=files)
    #r = session.post(url, files=files)
    r.raise_for_status()
    return r.json()

        
def _process_drawio_file(page_id: str,
                         att_id: str, filename: str, data: bytes, rewrite_cb: Callable[[str], Optional[str]]) -> str:
    """
    .drawio(xml): <mxfile><diagram>payload</diagram></mxfile>
    payloadì´ ì••ì¶•ì´ë©´ í•´ì œ â†’ ì¹˜í™˜ â†’ ì›í˜•(ì••ì¶•/í‰ë¬¸) ë³µì› í›„ ì—…ë¡œë“œ.
    """
    prefix = '"link="'

    body = data.decode("utf-8", errors="replace")
    org_body = body
    print(f"xml {body}")

    body = replace_links_spacekey(body, prefix)
    body = replace_links_tinyui(body, prefix, page_id)
    body = replace_links_page_id(body, prefix)
    body = replace_links_short_page_id(body, prefix)

    if(org_body != body):
        print(f"drawio file updated: {filename}")
        _upload_new_attachment_version(page_id, att_id, filename, body.encode("utf-8"), "application/xml")
        
    return body


def replace_links_drawio(body, page_json:Dict[str, Any]): 
    page_id = page_json.get("id")
    if not page_id:
        return body

    # 1) ì²¨ë¶€ ì¡°íšŒ (ë¼ë²¨/ë¯¸ë””ì–´íƒ€ì… í•¨ê»˜)
    atts = _list_attachments(page_id)

    # 2) draw.io í›„ë³´ë§Œ ì²˜ë¦¬
    for att in atts:
        filename = att.get("title") or (att.get("metadata", {}) or {}).get("filename") or ""
        labels = {lab["name"] for lab in (att.get("metadata", {}) or {}).get("labels", {}).get("results", [])}
        media  = (att.get("metadata", {}) or {}).get("mediaType", "") or ""
        low    = filename.lower()

        is_drawio_label     = "drawio" in labels
        is_drawio_mediatype = media in {"application/drawio", "application/vnd.jgraph.mxfile"}
        is_svg              = (media == "image/svg+xml") or low.endswith(".svg")

        if not (is_drawio_label or is_drawio_mediatype or is_svg or low.endswith(".drawio") or low.endswith(".drawio.svg")):
            continue

        try:
            data, ctype = _download_attachment_via_link(att)

            # .drawio (mxfile) ìŠ¤íƒ€ì¼
            if is_drawio_mediatype or low.endswith(".drawio") or _looks_like_mxfile(data):
                status = _process_drawio_file(
                    page_id, att["id"], filename, data, lambda url: _rewrite_single_url(url)
                )
            # .svg ìŠ¤íƒ€ì¼
            # elif is_svg or low.endswith(".drawio.svg"):
            #     status = _process_drawio_svg(
            #         session, BASE_URL, page_id, att["id"], filename, data,
            #         lambda url: _rewrite_single_url(url, session, BASE_URL, ORIGIN_SPACES, TARGET_SPACE)
            #     )
            # else:
            #     status = "skip"

            print(f" - draw.io attachment {filename or att.get('id')}: {status}")

        except Exception as e:
            print(f" - draw.io attachment {filename or att.get('id')}: error: {e}")

    return body


def resolve_short_url_to_title(short_url):
    """
    short URLì„ í’€ì–´ì„œ page ID ë°˜í™˜
    :param short_url: e.g. https://your-domain.atlassian.net/x/AbCdE
    :param auth: requests basic auth tuple (username, API token)
    """
    response = requests.get(short_url, headers=headers, allow_redirects=True)
    
    # ìµœì¢… ë¦¬ë””ë ‰ì…˜ URLì—ì„œ page ID ì¶”ì¶œ
    title = response.url.split('/')[-1].replace('+', ' ') # urlì—ì„œëŠ” ìŠ¤í˜ì´ìŠ¤ê°€ +ë¡œ ë‚˜ì™€ì„œ.
    return title
    
    return title

    
def resolve_tiny_url(short_url):
    response = requests.get(short_url, allow_redirects=False, headers=headers)
    if response.status_code in [301, 302]:
        return response.headers['Location']
    else:
        raise Exception(f"ë¦¬ë””ë ‰ì…˜ ì‹¤íŒ¨: status code {response.status_code}")

# 2. URLì—ì„œ page ID ì¶”ì¶œ (e.g. /pages/viewpage.action?pageId=12345678)
def extract_page_id(full_url):
    match = re.search(r"pageId=(\d+)", full_url)
    if match:
        return match.group(1)
    else:
        raise Exception(f"pageIdë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_url}")
    
def get_page_info_by_id(page_id):
    """
    page IDë¡œ title ë“± í˜ì´ì§€ ì •ë³´ ì¡°íšŒ
    """
    url = f"{BASE_URL}/rest/api/content/{page_id}"
    params = {"expand": "title"}
    response = requests.get(url, headers=headers, params=params)
    response.raise_for_status()
    return response.json()
    


def get_short_url_by_title(title, space_key, base_url):
    """
    ì£¼ì–´ì§„ titleë¡œ í˜ì´ì§€ë¥¼ ê²€ìƒ‰í•˜ê³  short URL(tiny link)ì„ ë°˜í™˜
    """
    # 1. ì œëª©ìœ¼ë¡œ í˜ì´ì§€ ì¡°íšŒ
    url = f"{base_url}/rest/api/search"
    params = {
        "title": title,
        "space": space_key,
        "expand": "version"  # title ì¡´ì¬ ìœ ë¬´ í™•ì¸ìš©
    }
    resp = requests.get(url, headers=headers, params=params)
    resp.raise_for_status()
    data = resp.json()
    
    # 2. ê²°ê³¼ ì—†ì„ ê²½ìš° ì²˜ë¦¬
    if not data.get("results"):
        print("Martin", "get_short_url_by_title", "í•´ë‹¹ í˜ì´ì§€ ì—†ìŒ {title}")
        return None  # or raise Exception("Page not found")
    
    page = data["results"][0]
    page_id = page["id"]
    
    # 3. í˜ì´ì§€ IDë¡œ tiny link ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    url = f"{base_url}/rest/api/content/{page_id}?expand=shortUrl,tinyui"
    resp = requests.get(url, auth=auth)
    resp.raise_for_status()
    page_data = resp.json()

    # 4. shortUrl í•„ë“œê°€ ì¡´ì¬í•  ê²½ìš° ë°˜í™˜
    tiny_url = page_data.get("tinyui", {}).get("link")
    if tiny_url:
        return f"{base_url}{tiny_url}"
    else:
        return None
    
def url_encode_query(query_string):
    """
    Encodes the given query string for use in a URL.

    Args:
    query_string (str): The query string to encode.

    Returns:
    str: The URL-encoded query string.
    """
    return urllib.parse.quote_plus(query_string)

def get_page_info_by_title(space_key, title):
    url = f"{BASE_URL}/rest/api/search"
    params = {
        'cql': f'title="{title}" AND space="{space_key}"',
        'limit': 10
    }
    
    response = requests.get(url, headers=headers, params=params)

    if response.status_code != 200:
        raise Exception(f"Request failed with status code {response.status_code}")
    
    data = response.json()
    results = data.get('results', [])
    
    if not results:
        return None
    
    for result in results:
        if result.get('title') == title:
            return {
                "id" :  result['content']['id'],
                "title" :  result['content']['title'],
                "webui" :  result['content']['_links']['webui'],
                "tinyui" :  result['content']['_links']['tinyui']
            }
    return None


def get_new_short_url(short_url, new_space):
    # Step 1: Extract the page ID from the short URL
    title = resolve_short_url_to_title(short_url)
    new_short_url = get_page_info_by_title(TARGET_SPACE, title)['tinyui']   

    if new_short_url:
        return f"{BASE_URL}{new_short_url}"
    else:
        return None
        

def update_page(pid, title):
    url = f"{BASE_URL}/rest/api/content/{pid}?expand=body.storage,version"
    res = requests.get(url, headers=headers)
    if res.status_code != 200:
        print(f"âŒ Failed to get {title}")
        return

    data = res.json()
    body = data['body']['storage']['value']
    version = data['version']['number']
    
    new_body = replace_links_spacekey(body)
    new_body = replace_links_tinyui(new_body, pid)
    new_body = replace_links_page_id(new_body)
    new_body = replace_links_drawio(new_body, data)

    if new_body == body:
        print(f"ğŸ” No change: {title}")
        return


    space = data['_expandable']['space'].strip('/').split('/')[-1] #space pathì˜ ë§¨ë§ˆì§€ë§‰ ê°€ì§€ê³  ì˜´
    payload = {
        "id": pid,
        "type": "page",
        "title": title,
        "space": {"key": space},
        # "body": {"storage": {"value": "hello world", "representation": "storage"}},
        "body": {"storage": {"value": new_body, "representation": "storage"}},
        "version": {"number": version + 1}
    }

    put_url = f"{BASE_URL}/rest/api/content/{pid}"
    put_res = requests.put(put_url, json=payload, headers=headers)
    print(f"{'âœ… Updated' if put_res.status_code == 200 else 'âŒ Failed'}: {title}")

def set_variables(mode) :
    global BASE_URL, PAGE_ID, ORIGIN_SPACES, TARGET_SPACE, TESTPAGE
    if mode == "TEST" :
        BASE_URL = TEST_BASE_URL
        PAGE_ID = "1127350378"
        ORIGIN_SPACES = ['TPG']
        TARGET_SPACE = 'ARU'

    if mode == "TEST-DRAWIO" :
        BASE_URL = TEST_BASE_URL
        PAGE_ID = "1128824849"
        ORIGIN_SPACES = ['TPG']
        TARGET_SPACE = 'ARU'
        TESTPAGE = "TechStack View - Draw.io"
if __name__ == "__main__":
#    test_short_url()
    # set_variables("TEST")
    set_variables("TEST-DRAWIO")
    update_page(PAGE_ID, TESTPAGE)

    # pages = get_child_pages(ROOT_PAGE_ID)
    # print(f"ğŸ” Pages under root {ROOT_PAGE_ID}: {len(pages)}")

    # for pid, title in pages:
    #     update_page(pid, title)
    #     time.sleep(0.5)

    filename = 'short_urls.csv'
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        fieldnames = ['old_url', 'new_url']
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        for old_url, new_url in short_urls.items():
            writer.writerow({'old_url': old_url, 'new_url': new_url})


    print(f"\nâš ï¸ Short URLs written to short_urls.csv: {len(short_urls)}")
